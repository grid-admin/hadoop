
%Finally we proceeded with the execution of the specific Big Data use cases described in Section \ref{ssect-execution}.

% Results Enc. Britannica
In Table \ref{table:britannica} we show the results obtained for the Encyclop{\ae}dia Britannica use case. All measurements are repeated 5 times and the average and standard deviation are displayed, allowing to have an estimation of the variability involved in the measurement. As it can be seen even if the block size used in this case is very small (1MB), the wordcount map reduce job scales really well and the overhead introduced by the federated cluster is small, being in general in order of 10-15\%, and 28\% in the worst case that corresponds to the 101 cluster. This is mainly due to the large overhead that mapreduce task creation and distribution imposes in this use case, especially in the larger cluster sizes that involve more reduce tasks--we are using a number of reduce tasks equal to the 95\% of the total number of slaves.

As expected the put times are much lower in one-site-only scenario because in this case the connection between all the VMs is much faster because all of them are generally located in the same data centre.

%% Table results Enc. Britannica
\begin{table}[h!]
\caption{Benchmark times obtained for the Encyclop{\ae}dia Britannica use case. Both the average time and standard deviation are reported. Federated times where measured in a federated cluster that included resources both from CESGA and CESNET; all one-site-only times where obtained in a one site deployment at CESGA except the 10 node cluster indicated in the second row that run at CESNET. }
\label{table:britannica}
%
\vspace{-0.5em}
%
\begin{center}
\begin{tabular}{ccccc}
\toprule
%Parameter				& Type\tablenotemark[a] & Value	 	\\
%Cluster size				& put (s) Local		& put (s)		& wordcount (s) 	& wordcount (s) 	\\
					& \multicolumn{2}{c}{Federated} 		& \multicolumn{2}{c}{One-site-only} \\
Cluster size				& put (s)		& wordcount (s) 	& put (s)		& wordcount (s)\\
\midrule
10 (CESGA)             			&        		&               	& $47\pm2$		& $169\pm1$\\
10 (CESNET)          			& 			&  			& $9.5\pm0.2$		& $160\pm1$\\
21                   			& $235\pm12$		& $108\pm2$     	& $37\pm1$		& $97\pm1$\\
31                   			& $189\pm4$		& $90\pm2$      	& $36\pm2$		& $78\pm2$\\
41                   			& $190\pm11$		& $81\pm4$      	& $33\pm2$		& $71\pm3$\\
51                   			& $133\pm7$		& $73\pm3$      	& $33\pm1$		& $66\pm2$\\
101                  			& $94\pm7$		& $67\pm22$      	& $33\pm4$		& $52\pm5$\\
%
\bottomrule
\multicolumn{5}{c}{\rule{0.98\textwidth}{0em}}\\
\rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} &  \rule{0.2\textwidth}{0cm} & \\
\end{tabular}
\end{center}
%\vspace{-3.5em}
%\tablenotetext[a] {Identifies the configuration file where the parameter is set, the complete configuration files can be found at \url{https://github.com/grid-admin/hadoop}}
\end{table}



% Results Wikipedia
In Table \ref{table:wikipedia} we show the results obtained for the Wikipedia use case. It should be noted that due to the high duration of the transfers, we did not repeat the put measurements 5 times, and only the wordcount measurements were repeated. Also due to the longer duration of the benchmarks we only used the two larger cluster sizes: 51 and 101.

In this case the wordcount mapreduce results are quite surprising because the federated cluster instance is able to outperform the one-site-only instance in both scenarios. This is attributed mainly to the faster execution of the benchmark in CESNET nodes compared to CESGA nodes--CESNET VMs use Xen paravirtualization and a faster CPU whereas CESGA VMs use KVM and QEMU--. In this case the relative overhead due to mapreduce task creation is much lower than in the previous use case because we are using a much larger block size of 64MB.

The put times are much larger in the federated cluster due to the fact that the amount of data to transfer is much larger--HDFS stores three copies of the Wikipedia, 42GB each, so that the available bandwidth between the VMs and the UI located at CESGA plays a key role, being much lower in the case of CESNET VMs.

%% Table results Wikipedia
\begin{table}[h!]
\caption{Benchmark times obtained for the Wikipedia use case. Both the average time and standard deviation are reported except for the put times that where only measured once for each deployment. Federated times where measured in a federated cluster that included resources both from CESGA and CESNET; all one-site-only times where obtained in a one site deployment at CESGA. }
\label{table:wikipedia}
%
\vspace{-0.5em}
%
\begin{center}
\begin{tabular}{ccccc}
\toprule
%Parameter				& Type\tablenotemark[a] & Value	 	\\
%Cluster size				& put (s) Local		& put (s)		& wordcount (s) 	& wordcount (s) 	\\
    					& \multicolumn{2}{c}{Federated} 		& \multicolumn{2}{c}{One-site-only} \\
Cluster size				& put (s)		& wordcount (s) 	& put (s)		& wordcount (s)\\
\midrule
51                   			& $19190$		& $1001\pm39$      	& $6705$		& $1347\pm117$\\
101                  			& $13208$		& $705\pm14$      	& $5665$		& $725\pm18$\\
%
\bottomrule
\multicolumn{5}{c}{\rule{0.98\textwidth}{0em}}\\
\rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} &  \rule{0.2\textwidth}{0cm} & \\
\end{tabular}
\end{center}
%\vspace{-3.5em}
%\tablenotetext[a] {Identifies the configuration file where the parameter is set, the complete configuration files can be found at \url{https://github.com/grid-admin/hadoop}}
\end{table}


% Results Teragen/terasort
Finally, in Table \ref{table:terasort} we show the results obtained for the Teragen and Terasort benchmarks. In this case we can generate much larger data sets than the Wikipedia, so we start at 50GB and go up to 2TB. We can see the scaling is good but after reaching 600GB the TeraSort benchmark fails because there is no enough space--total storage available is 2.65GB--to store the intermediate data. In order to evaluate larger data sets we reduced the number of HDFS replicas to one--reducing them to two would not provide much additonal margin--and rerun the whole set of benchmarks. In this way, we could reach 2TB in the Teragen benchmark. Unfortunately having just one replica eliminates the fault tolerance of the system, making the system much less reliable, and causing the larger Terasort benchmarks to consistently fail.

%% Table results teragen/terasort
\begin{table}[h!]
\caption{Results obtained for the TeraGen and TeraSort benchmarks: times where measured in a federated cluster that included 20 slave nodes running at CESGA and 20 at CESNET with the master node at CESGA.  }
\label{table:terasort}
%
\vspace{-0.5em}
%
\begin{center}
\begin{tabular}{cccc}
\toprule
Dataset size		& replicas & teragen (s)		& terasort (s) 	  \\
\midrule
% 3 replicas
50GB  & 3 & 1342 & 750 \\
100GB & 3 & 2510 & 1504 \\
200GB & 3 & 4575 & 3278 \\
300GB & 3 & 5934 & 5766 \\
400GB & 3 & 7771 & 7244 \\
600GB & 3 & 11595 & Failed \\
800GB & 3 & 15351 & Failed \\
% 1 replica
50GB & 1 & 86 & 820 \\
100GB & 1 & 140 & 1506 \\
200GB & 1 & 272 & 3472 \\
300GB & 1 & 379 & 5529 \\
400GB & 1 & 538 & 7679 \\
500GB & 1 & 624 & 10094 \\
600GB & 1 & 715 & Failed \\
800GB & 1 & 1014 & Failed \\
%
\bottomrule
\multicolumn{4}{c}{\rule{0.98\textwidth}{0em}}\\
\rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} & \rule{0.2\textwidth}{0cm} &   \\
\end{tabular}
\end{center}
%\vspace{-3.5em}
%\tablenotetext[a] {Identifies the configuration file where the parameter is set, the complete configuration files can be found at \url{https://github.com/grid-admin/hadoop}}
\end{table}




